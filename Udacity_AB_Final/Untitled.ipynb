{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing to Test Approaches to Reducing Early Udacity Course Cancellation\n",
    "\n",
    "<strong>Description (from Udacity) </strong>\n",
    "    \n",
    "At the time of this experiment, Udacity courses currently have two options on the course overview page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "\n",
    "\n",
    "In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. This screenshot shows what the experiment looks like.\n",
    "\n",
    "\n",
    "The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough timeâ€”without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.\n",
    "\n",
    "<strong>The new screener question</strong>\n",
    "![title](screen_shoot.jpg)\n",
    "\n",
    "<strong>Project Instructions</strong>\n",
    "https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eperimental Design\n",
    "\n",
    "<strong>Unit of Diversion</strong>\n",
    "For the experiment the unit of diversion will be a cookie, however, if a student enrolls in the free trail they will be tracked via user-id from that point forward. The same user-id cannot enroll in the free trail twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they are signed in when they visit the course overview page.\n",
    "\n",
    "## Metric Selection\n",
    "\n",
    "### Invariant Metrics\n",
    "Theses metrics should remain roughly the same between the control and expirement group, and they should also be inline with the baseline measures for each. We expect a similiar distribution for these metrics.\n",
    "<ul>\n",
    "    <li>Number of Cookies: The number of unique cookies to view the course overview page</li>\n",
    "    <li>\n",
    "    Number of Clicks: The number of unique cookies to click the \"Start Free Trail\" button (which happens after the free trial screener is triggered)</li>\n",
    "    <li>\n",
    "    Click Through Probability: The number of unique cookies to click the \"Start Free Trial\" button divided by the number of unique cookies to view the course overview page. The experimental screen question will be shown AFTER a user clicks the \"start free trail\" button, as a result for both control and experimental groups the experience and thus the probability of click should remain roughly the same. \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "### Evaluation Metrics\n",
    "We will analyze changes in these metrics to determine the effectiveness of the free trial screener at reducing cancellation rates.\n",
    "<ul>\n",
    "    <li>Gross Conversion: The number of user-ids to complete checkout and enroll in the free trial divided by the number of unique cookies to click the \"Start Free Trial\" button.</li>\n",
    "    <li>Retention: The number of user-ids to remain enrolled past the 14-day boundary (and make at least 1 payment) divided by the number of user-ids to complete checkout</li>\n",
    "    <li>Net Conversion: The number of user-id to remain enrolled past the 14-day boundary (and make at least 1 payment) divided by the number of unique cookies to click the \"Start Free trail\" button.</li>\n",
    "</ul>\n",
    "\n",
    "## Calculating Standard Deviation\n",
    "For each evaluation, using the baseline data provided by Udacity, the analytical estimate of standard deviation is calculated for a sample size of 5,000 cookies visiting the course overview page.\n",
    "\n",
    "Get the baseline data from Udacity provided CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unique_cookie_views': 40000.0, 'unique_cookies_button_click': 3200.0, 'enrollments_day': 660.0, 'click_probability': 0.08, 'prob_enrolling_g_click': 0.20625, 'prob_payment_g_enroll': 0.53, 'prob_payment_g_click': 0.1093125, 'n_sample': 5000}\n"
     ]
    }
   ],
   "source": [
    "# get data into dataframe\n",
    "df_baseline = pd.read_csv('baseline_data.csv')\n",
    "df_baseline.head()\n",
    "\n",
    "lst_baselineMetrics = [\n",
    "    'unique_cookie_views',\n",
    "    'unique_cookies_button_click', \n",
    "    'enrollments_day',\n",
    "    'click_probability',\n",
    "    'prob_enrolling_g_click', \n",
    "    'prob_payment_g_enroll', \n",
    "    'prob_payment_g_click'\n",
    "]\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# set values\n",
    "for i,j in df_baseline.iterrows():\n",
    "    metric = lst_baselineMetrics[i]\n",
    "    val = j[1]\n",
    "    metrics[metric] = val\n",
    "\n",
    "# given by Udacity    \n",
    "metrics['n_sample'] = 5000\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Analytical standard deviation estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std Gross Conversion: 0.020230604137049392\n",
      "Std Retentoin: 0.05494901217850908\n",
      "Std Net Conversion: 0.01560154458248846\n"
     ]
    }
   ],
   "source": [
    "# set variables\n",
    "n_clicks = metrics['unique_cookies_button_click']\n",
    "gross_conversion = metrics['prob_enrolling_g_click']\n",
    "click_probability = metrics['click_probability']\n",
    "n_sample = metrics['n_sample']\n",
    "retention = metrics['prob_payment_g_enroll']\n",
    "net_conversion = metrics['prob_payment_g_click']\n",
    "enrollments_day = metrics['enrollments_day']\n",
    "page_views = metrics['unique_cookie_views']\n",
    "\n",
    "# gross conversion estimate\n",
    "std_gross_conversion = math.sqrt((gross_conversion * (1-gross_conversion))/(click_probability*n_sample))\n",
    "\n",
    "# retention std estimate\n",
    "std_retention = math.sqrt((retention*(1-retention))/(enrollments_day/page_views*n_sample))\n",
    "\n",
    "# Net conversion std estimate\n",
    "std_net_conversion = math.sqrt((net_conversion*(1-net_conversion))/(n_clicks/page_views*n_sample))\n",
    "\n",
    "print(f'Std Gross Conversion: {std_gross_conversion}')\n",
    "print(f'Std Retentoin: {std_retention}')\n",
    "print(f'Std Net Conversion: {std_net_conversion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Sizing\n",
    "\n",
    "Using the online calculator found <a href='https://www.evanmiller.org/ab-testing/sample-size.html'>here</a> the sample size required is calculated. Assuming an alpha value of 0.05 and beta value of 0.2 for each metric.\n",
    "\n",
    "### Gross Conversion\n",
    "> User IDs to complete checkout and enroll/ # of unique cookies to click start free trial button\n",
    "\n",
    "<ul>\n",
    "    <li>Baseline Conversion: 20.625%</li>\n",
    "    <li>Minimum Detectable Effect: 1%</li>\n",
    "    <li>Alpha: 0.05</li>\n",
    "    <li>Beta: 1.0-0.2=0.8</li>\n",
    "    <li>Number of Groups: 2 (experimental and Control)</li>\n",
    "    <li>Sample Size per Group: 25,835</li>    \n",
    "    <li>Total Sample Size: 2 x 25,835 = 51,670</li>\n",
    "    <li>Pageviews: We need 51,670 total individuals to click the \"Start Free Trail\" button, with an 8% click/pageview rate we need:51,670/.08 = 645,875 page views</li>\n",
    "     <li>Duration Required (Assuming 100% of traffic): 645,875/40,000 = 16</li>\n",
    "</ul>\n",
    "\n",
    "### Retention\n",
    "> Number of User IDs to remain enrolled past 14-day boundary/ Number of User IDs to complete checkout\n",
    "<ul>\n",
    "    <li>Baseline Retention: 53.0%</li>\n",
    "    <li>Minimum Detectable Effect: 1%</li>\n",
    "    <li>Alpha: 0.05</li>\n",
    "    <li>Beta: 1.0-0.2=0.8</li>\n",
    "    <li>Number of Groups: 2 (Experimental and Control)</li>\n",
    "    <li>Sample Size per Group: 39,115</li>\n",
    "    <li>Total Sample Size: 39,115 X 2 = 78,230 </li>\n",
    "    <li>Pageviews: 78,230/(.08*.20625) = 4,741,212</li>   \n",
    "    <li>Duration Required (Assuming 100% of traffic): 4,741,212/40,000 = 119</li>\n",
    "</ul>\n",
    "\n",
    "### Net Conversion\n",
    "> Number of User IDs to remain enrolled past the 14-day boundary/number of unique cookies to click the \"Start Free Trial\" button\n",
    "\n",
    "<ul>\n",
    "    <li>Baseline Net Conversion:10.93125%</li>\n",
    "    <li>Minimum Detecable Effect: 1%</li>\n",
    "    <li>Alpha: 0.05</li>\n",
    "    <li>Beta: 1.0-0.2 = 0.8</li>\n",
    "    <li>Number of Groups: 2 (Experimental and Control)</li>\n",
    "    <li>Sample Size per Group: 15,464</li>\n",
    "    <li>Total Sample Size: 30,928</li>\n",
    "    <li>Pageviews:30,928/.08 = 685,325</li>\n",
    "    <li>Duration Required (Assuming 100% of traffic): 685,325/40,000 = 17</li>\n",
    "</ul>\n",
    "\n",
    "As we are reviewing various metrics, we need to use the largest number of pageviews required by any given metric. In this case, the Retention metric requires the most number of pageviews at 4,741,212 views, which means the total number of pageviews required for are testing is 4,741,212.\n",
    "\n",
    "## Duration and Exposure\n",
    "\n",
    "With the baseline value of pageviews at 40,000 page views per day, and diverting 100% of all site traffic, it would take roughly 119 days to collect enough data for the experiment. Best practice typically calls for 1%-10% of traffic to both allow other experiments and to minimize customer impact, however, the nature of the change is fairly subtle it does not present a significant risk in terms of degraded user experience. That said, an experiment of 118 days still creates other issues, and the duration of the experiment can be scalled back to roughly 17 days if we focus net conversions and gross conversions, or 16 days if we limit the scope of investigation excusively on gross conversions. \n",
    "\n",
    "## Experiment Results Analysis\n",
    "### Sanity Checks \n",
    "> Prior to analyzing experiment results, we will review the invariant metrics to smell check the overall execution of the experiment. \n",
    "\n",
    "<i>Analysis Conducted in Google Sheet, but summary table provided below</i>\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <td></td>\n",
    "            <th>Number of Cookies</th>\n",
    "            <th>Number of Clicks</th>\n",
    "            <th> Click-Through-Probability</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Control</th>\n",
    "            <td>345,543 (50.06%)</td>\n",
    "            <td>28,378 (50.05%)</td>\n",
    "            <td>8.2126%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Experiment</th>\n",
    "            <td>344,660 (49.94%)</td>\n",
    "            <td>28,325 (49.95%)</td>\n",
    "            <td>8.2182%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Total</th>\n",
    "            <td>690,203</td>\n",
    "            <td>56,703</td>\n",
    "            <td>8.2154%</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "For all items, a 95% confidence internval is used.\n",
    "\n",
    "#### Number of Cookies\n",
    "> Overall we expect the number of cookies in the control and experimental group to be roughly equal\n",
    "<ul>\n",
    "    <li>Standard Error:0.0006018407403</li>\n",
    "    <li>Margin of Error:0.001179586176</li>\n",
    "    <li>Lower Bound:0.4988204138</li>\n",
    "    <li>Upper Bound:0.5011795862</li>\n",
    "    <li>Observed: 50.06% (control)</li>\n",
    "    <li>Pass Sanity Check?:Yes</li>\n",
    "</ul>\n",
    "\n",
    "#### Number of clicks on \"Start Free Trial\"\n",
    "> Overall we expect to be roughly equal across both groups\n",
    "<ul>\n",
    "    <li>Standard Error:0.00209974708</li>\n",
    "    <li>Margin of Error:0.004115428656</li>\n",
    "    <li>Lower Bound:0.4958845713</li>\n",
    "    <li>Upper Bound:0.5041154287</li>\n",
    "    <li>Observed: 50.05% (control)</li>\n",
    "    <li>Pass Sanity Check?:Yes</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "#### Click-through-probability on \"start Free Trial\"\n",
    "> Overall we expect to be roughly equal across both groups\n",
    "<ul>\n",
    "    <li>Standard Error:0.0006610608156</li>\n",
    "    <li>Margin of Error:0.001295655391</li>\n",
    "    <li>Lower Bound:-0.0013</li>\n",
    "    <li>Upper Bound:0.0013</li>\n",
    "    <li>Observed: 0.0001 (experiment-control)</li>\n",
    "    <li>Pass Sanity Check?:Yes</li>\n",
    "</ul>\n",
    "\n",
    "For Number of Cookies and Clicks, standard error is calculated as follows:\n",
    "SQRT((0.5X0.5)/(N1 + N2))\n",
    "Where N1 and N2 represent the number of cookies for the control and experimental group, respectively.\n",
    "\n",
    "For Click Through Probability, standard error is calculated as follows:\n",
    "SQRT(PGX(1-PG)X((1/N1) + (1/N2)))\n",
    "Where N1 and N2 are the same as above, and PG if the Global Pooled rate of click through probability (56,703/690,203)\n",
    "\n",
    "### Effect Size Test\n",
    "> Now that we have passed the required sanity checks to ensure our experiment was executed correctly, we can now determine the impact of the screener question on the Gross Conversion and Net Conversion rates.\n",
    "\n",
    "### Calculate Conversion Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "df_control = pd.read_csv('Final_Results_Control.csv')\n",
    "df_exp = pd.read_csv('Final_Results_Experiment.csv')\n",
    "\n",
    "# make df copy\n",
    "df_control_copy = df_control.copy()\n",
    "df_exp_copy = df_exp.copy()\n",
    "\n",
    "# drop na's\n",
    "df_control_copy = df_control_copy.dropna()\n",
    "df_exp_copy = df_exp_copy.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control, Gross: 0.2188746891805933 Net: 0.11756201931417337\n",
      "Experimental, Gross: 0.19831981460023174 Net: 0.1126882966396292\n",
      "Global, Gross: 0.20860706740369866 Net: 0.20860706740369866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculate gross and net conversion rates\n",
    "df_control_copy['gross_conversion'] = df_control_copy['Enrollments']/df_control_copy['Clicks']\n",
    "df_control_copy['net_conversion'] =  df_control_copy['Payments']/df_control_copy['Clicks']\n",
    "\n",
    "df_exp_copy['gross_conversion'] = df_exp_copy['Enrollments']/df_exp_copy['Clicks']\n",
    "df_exp_copy['net_conversion'] =  df_exp_copy['Payments']/df_exp_copy['Clicks']\n",
    "\n",
    "# calculate total and pooled global rates\n",
    "gross_c = df_control_copy['Enrollments'].sum()/df_control_copy['Clicks'].sum()\n",
    "net_c = df_control_copy['Payments'].sum()/df_control_copy['Clicks'].sum()\n",
    "\n",
    "gross_e = df_exp_copy['Enrollments'].sum()/df_exp_copy['Clicks'].sum()\n",
    "net_e = df_exp_copy['Payments'].sum()/df_exp_copy['Clicks'].sum()\n",
    "\n",
    "# pooled Global\n",
    "global_clicks = df_control_copy['Clicks'].sum()+df_exp_copy['Clicks'].sum()\n",
    "gross_global = (df_control_copy['Enrollments'].sum() + df_exp_copy['Enrollments'].sum())/global_clicks\n",
    "net_global = (df_control_copy['Payments'].sum() + df_exp_copy['Payments'].sum())/global_clicks\n",
    "\n",
    "# print Gross and Net conversion rates for control, experimental, and global pooled\n",
    "print(f'Control, Gross: {gross_c} Net: {net_c}')\n",
    "print(f'Experimental, Gross: {gross_e} Net: {net_e}')\n",
    "print(f'Global, Gross: {gross_global} Net: {gross_global}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Standard Error and Analyze Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE Gross Conversion: 0.004371675385225936 MOE: 0.008568483755042836\n",
      "SE Net Conversion: 0.0034341335129324238 MOE: 0.0067309016853475505\n"
     ]
    }
   ],
   "source": [
    "# set Z value to 1.96 to correspond to alpha of .05\n",
    "z_value = 1.96\n",
    "clicks_c = df_control_copy['Clicks'].sum()\n",
    "clicks_e = df_exp_copy['Clicks'].sum()\n",
    "# Standard Error and margin of error, Gross Conversion\n",
    "se_gross = math.sqrt((gross_global*(1-gross_global))*((1/clicks_c)+(1/clicks_e)))\n",
    "moe_gross = z_value * se_gross\n",
    "# Standard Error and margin of error, Net Conversion\n",
    "se_net = math.sqrt((net_global*(1-net_global))*((1/clicks_c)+(1/clicks_e)))\n",
    "moe_net = z_value * se_net\n",
    "print(f'SE Gross Conversion: {se_gross} MOE: {moe_gross}')\n",
    "print(f'SE Net Conversion: {se_net} MOE: {moe_net}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gross Conversion diff -0.020554874580361565, CI: [-0.0291233583354044,-0.01198639082531873]\n",
      "Statistically signfificant, CI does not include 0\n",
      "Net Conversion diff -0.0048737226745441675, CI: [-0.011604624359891718,0.001857179010803383]\n"
     ]
    }
   ],
   "source": [
    "# diff, gross conversion\n",
    "d_gross = gross_e - gross_c\n",
    "# establish 95% confidence interval around d based on MOE\n",
    "gross_lower =d_gross-moe_gross\n",
    "gross_upper = d_gross+moe_gross\n",
    "print('Gross Conversion diff {0}, CI: [{1},{2}]'.format(d_gross,gross_lower,gross_upper))\n",
    "if gross_lower > 0 or gross_upper <0:\n",
    "    print('Statistically signfificant, CI does not include 0')\n",
    "\n",
    "# diff, net\n",
    "d_net = net_e - net_c\n",
    "net_lower = d_net-moe_net\n",
    "net_upper = d_net+moe_net\n",
    "print('Net Conversion diff {0}, CI: [{1},{2}]'.format(d_net,net_lower,net_upper))\n",
    "if net_lower > 0 or net_upper <0:\n",
    "    print('Statistically signfificant, CI does not include 0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign Test\n",
    "Statistical signfificance of sign test was calculated using the online calculator here: https://www.graphpad.com/quickcalcs/binomial1.cfm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count Gross Control>Exp: 19, Count Net Control>Exp: 13, N: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aange\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\aange\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\aange\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# create new subset data frame \n",
    "df_sign_test = df_control_copy[['Date','gross_conversion','net_conversion']]\n",
    "df_sign_test['gross_conversion_exp'] = df_exp_copy['gross_conversion']\n",
    "df_sign_test['net_conversion_exp'] = df_exp_copy['net_conversion']\n",
    "\n",
    "# calculate diff in control group minus experimental group\n",
    "df_sign_test['gross_control_m_exp'] = df_sign_test['gross_conversion'] - df_sign_test['gross_conversion_exp']\n",
    "df_sign_test['net_control_m_exp'] = df_sign_test['net_conversion'] - df_sign_test['net_conversion_exp']\n",
    "\n",
    "# get diff values to a list\n",
    "lst_gross = df_sign_test['gross_control_m_exp'].values.tolist()\n",
    "lst_net = df_sign_test['net_control_m_exp'].values.tolist()\n",
    "\n",
    "n_gross = len(lst_gross)\n",
    "n_net = len(lst_net)\n",
    "\n",
    "assert n_gross == n_net\n",
    "\n",
    "# get counts of where control > experimental\n",
    "gross_control_positive = sum([1 for i in lst_gross if i > 0])\n",
    "net_control_positive = sum([1 for i in lst_net if i >0])\n",
    "\n",
    "print(f' Count Gross Control>Exp: {gross_control_positive}, Count Net Control>Exp: {net_control_positive}, N: {n_gross}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Review and Recommendation: Implement Change\n",
    "\n",
    "The goal of this analysis was to use the framework of an A/B test to determine the impact of adding a new screen question that appears after a user clicks the 'start free trial' button on the course overview page. The motive of the new screener question is to reduce the number of students who enroll in a program, but end up dropping out before the 14-day trial by providing students with a greater understanding of the time commitment required by the program--with the underlying assumption being that students with less time will opt out of signing up for the 14-day free trial and only more serious students will commit.\n",
    "\n",
    "Prior to beginning any analysis, we established some invariant metrics--number of cookies, number of button clicks, and probability of click--to allow us to determine if the overall experiment was implemented correctly. Afterwards, we selected 2 evaluation metrics to help determine the effectiveness of the new screener question, we selected Gross Conversion Rate and Net Conversion Rate. Additionally, we reviewed baseline values for a number of metrics to determine the exposure and duration of the experiment. \n",
    "\n",
    "The experiment was conducted and the final data was collected and organized into a CSV that was then imported in python for analysis. The analysis started with a basic sanity check on the invariant metrics of choice where we calculated the Standard Error and Margin of Error values for Number of Cookies, number of button clicks, and click-through-probability to then form confidence intervals, which enabled us to compare the value of these metrics between control and experimental group. The results showed that the invariant metrics between the control and experimental group were comparable, and suggest that the experiment was carried out correctly. Finally, we analyzed data corresponding to the effectiveness metrics of choice--Gross and Net Conversion rates. To analyze the result we: \n",
    "<ul>\n",
    "    <li>Calculated Global Pooled Gross and Net Conversion Rates</li>\n",
    "    <li>Calculated Standard Error Rates</li>\n",
    "    <li>With Z value of 1.96, Calculated Margin of Error</li>\n",
    "    <li>Calculated D Hat (difference in gross and net conversion rates between Control and Experimental groups)</li>\n",
    "    <li>Formed 95% confidence intervals for each evaluation metric</li>\n",
    "    <li>Evaluated D Hat for statistical and practical significance</li>\n",
    "</ul>\n",
    "\n",
    "The final results showed that the screener question had an effect on Gross Conversion Rates that was both statistically and practically significant. No such effect was found with respect to Net Conversion Rates. Additionally, the non-parametric sign test agreed with the overall results generated by the confidence intervals. \n",
    "\n",
    "At a high level reducing gross conversion rates seems bad, however, as the goal is to reduce overall dropout rates after the 14-day window by decreasing the number of time constrained students. Retention--a metric not analyzed in this study due to time constraints--most closely measures this overall process, and it is defined as user IDs enrolled past the 14-day period divided by number of users who complete checkout. Reducing overall gross conversion rates by reducing the number of time contrained students will reduce overall number of user ids to complete checkout (ie denominator of Retention), but also likely increase the numberator as time constrained students self-select out and thus leave a higher pool of committed students. That said, the change did not have the anticipated change on Net Conversion rates, as there was a small and statistically insingificant chagne in net conversion rates. Overall, I recommend implementing the change and continue gathering and analyzing data as it relates to retetion and net conversion. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
